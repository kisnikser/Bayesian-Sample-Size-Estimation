\section{Достаточный размер выборки не превосходит доступный}\label{sec2}

В этом разделе будем считать, что достоверно $m^* \leqslant m$. Это означает, что нам нужно просто формализовать, какой размер выборки можно считать достаточным.

\subsection{Анализ поведения функции правдоподобия}

Для определения достаточности будем использовать функцию правдоподобия. Когда в наличии имеется достаточно объектов, вполне естественно ожидать, что от одной реализации выборки к другой полученная оценка параметров не будет сильно меняться \cite{Joseph1997, Joseph1995}. То же можно сказать и про функцию правдоподобия. Таким образом, сформулируем, какой размер выборки можно считать достаточным.

\begin{definition}
    \label{sufficient-variance}
    Зафиксируем некоторое положительное число $\varepsilon > 0$. Размер выборки $m^*$ называется \textbf{D-достаточным}, если для всех $k \geqslant m^*$
    \[ D(k) = \mathbb{D}_{\mathfrak{D}_k} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \leqslant \varepsilon. \]
\end{definition}

С другой стороны, когда в наличии имеется достаточно объектов, также вполне естественно, что при добавлении очередного объекта в рассмотрение полученная оценка параметров не будет сильно меняться. Сформулируем еще одно определение.

\begin{definition}
    \label{sufficient-difference}
    Зафиксируем некоторое положительное число $\varepsilon > 0$. Размер выборки $m^*$ называется \textbf{M-достаточным}, если для всех $k \geqslant m^*$ 
    \[ M(k) = \left| \mathbb{E}_{\mathfrak{D}_{k+1}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E}_{\mathfrak{D}_k} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \right| \leqslant \varepsilon. \]
\end{definition}
\begin{note}
    В определениях \ref{sufficient-variance} и \ref{sufficient-difference} вместо функции правдоподобия $L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$ можно рассматривать ее логарифм $l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$.
\end{note}

Предположим, что $\mathbb{W} = \mathbb{R}^n$, т.е. параметры $\bw$ представляются в виде вектора. Напомним, что информацией Фишера называется матрица
\[ \left[\mathcal{I}(\bw)\right]_{ij} = - \mathbb{E}\left[ \dfrac{\partial^2 \log p(\by | \bx, \bw)}{\partial w_i \partial w_j} \right]. \]
Достаточно известным результатом является асимптотическая нормальность оценки максимума правдоподобия. 

\begin{proposition}
    Пусть $\hat{\bw}_k$ есть оценка максимума правдоподобия параметров $\bw$. Тогда при определенных условиях регулярности (которые на практике чаще всего выполнены) имеет место следующая сходимость по распределению:
    \[ \sqrt{k}\left( \hat{\bw}_k - \bw \right) \xrightarrow{d} \mathcal{N}\left(0, \mathcal{I}^{-1}(\bw)\right), \]
    откуда следует
    \[ \hat{\bw}_k \xrightarrow{d} \mathcal{N}\left(\bw, \left[k\mathcal{I}(\bw)\right]^{-1}\right). \]
\end{proposition}

Из сходимости по распределению в общем случае не следует сходимость моментов случайного вектора. Тем не менее, если предположить последнее, то в некоторых моделях можно доказать корректность предложенного нами определения M-достаточного размера выборки.

Для удобства обозначим параметры распределения $\hat{\bw}_k$ следующим образом: математическое ожидание $\mathbb{E}_{\mathfrak{D}_k}\hat{\bw}_k = \bm_k$ и матрица ковариации $\mathbb{D}\hat{\bw}_k = \bSigma_k$. Тогда имеет место следующая теорема, доказательство которой приведено в разделе \ref{append}.

\begin{theorem}[Киселев, 2023]\label{theorem1}
    Пусть $\| \bm_{k+1} - \bm_k \|_2 \to 0$ и $\| \bSigma_{k+1} - \bSigma_k \|_{F} \to 0$ при $k \to \infty$. Тогда в модели линейной регрессии определение M-достаточного размера выборки является корректным. А именно, для любого $\varepsilon > 0$ найдется такой $m^*$, что для всех $k \geqslant m^*$ выполнено $M(k) \leqslant \varepsilon$.
\end{theorem}

\begin{corollary}
    Пусть $\| \bm_k - \bw \|_2 \to 0$ и $\| \bSigma_k - \left[k\mathcal{I}(\bw)\right]^{-1} \|_{F} \to 0$ при $k \to \infty$. Тогда в модели линейной регрессии определение M-достаточного размера выборки является корректным. 
\end{corollary}

По условию задана одна выборка. Поэтому в эксперименте нет возможности посчитать указанные в определениях математическое ожидание и дисперсию. Для их оценки воспользуемся техникой бутстрап. А именно, сгенерируем из заданной $\mathfrak{D}_m$ некоторое число $B$ подвыборок размера $k$ с возвращением. Для каждой из них получим оценку параметров $\hat{\mathbf{w}}_{k}$ и посчитаем значение $L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$. Для оценки будем использовать выборочное среднее и несмещенную выборочную дисперсию (по бутстрап-выборкам).

Предложенные выше определения можно применять и в тех задачах, когда минимизируется произвольная функция потерь, а не максимизируется функция правдоподобия. Мы не приводим никаких теоретических обоснований этого, однако на практике такая эвристика оказывается достаточно удачной.

\subsection{Анализ апостериорного распределения параметров модели}

В работе \citep{MOTRENKO2014743} предлагается использовать дивергенцию Кульбака-Лейблера для оценки достаточного размера выборки в задаче бинарной классификации. Идея основывается на том, что если две подвыборки отличаются друг от друга на один объект, то полученные по ним апостериорные распределения должны быть близки. Эта близость определяется дивергенцией Кульбака-Лейблера. 

В настоящей работе предлагается развить этот подход, исследовать его не только в задаче классификации, но и в задаче регрессии. В качестве меры близости предлагается использовать не только дивергенцию Кульбака-Лейблера, но и функцию сходства s-score из [Адуенко].

Рассмотрим две подвыборки $\mathfrak{D}^1 \subseteq \mathfrak{D}_m$ и $\mathfrak{D}^2 \subseteq \mathfrak{D}_m$. Пусть $\mathcal{I}_1 \subseteq \mathcal{I} = \{ 1, \ldots, m \}$ и $\mathcal{I}_2 \subseteq \mathcal{I} = \{ 1, \ldots, m \}$~--- соответствующие им подмножества индексов.

\begin{definition}
    Подвыборки $\mathfrak{D}^1$ и $\mathfrak{D}^2$ называются \textbf{схожими}, если $\mathcal{I}_2$ может быть получено из $\mathcal{I}_1$ удалением, заменой или добавлением одного элемента, то есть $$ \left| \mathcal{I}_1 \triangle \mathcal{I}_2 \right| = \left| \left( \mathcal{I}_1 \setminus \mathcal{I}_2 \right) \cup \left( \mathcal{I}_2 \setminus \mathcal{I}_1 \right) \right| = 1. $$
\end{definition}

Рассмотрим две схожие подвыборки $\mathfrak{D}_k = (\bX_k, \by_k)$ и $\mathfrak{D}_{k+1} = (\bX_{k+1}, \by_{k+1})$ размеров $k$ и $k+1$ соответственно. Это означает, что большая из них получена добавлением одного элемента к меньшей. Найдем апостериорное распределение параметров модели по этим подвыборкам:
\[ p_k(\bw) = p(\bw | \mathfrak{D}_k) = \dfrac{p(\mathfrak{D}_k | \bw) p(\bw)}{p(\mathfrak{D}_k)} \propto p(\mathfrak{D}_k | \bw) p(\bw), \]
\[ p_{k+1}(\bw) = p(\bw | \mathfrak{D}_{k+1}) = \dfrac{p(\mathfrak{D}_{k+1} | \bw) p(\bw)}{p(\mathfrak{D}_{k+1})} \propto p(\mathfrak{D}_{k+1} | \bw) p(\bw). \]

\begin{definition}
    Зафиксируем некоторое положительное число $\varepsilon > 0$. Размер выборки $m^*$ называется \textbf{KL-достаточным}, если для всех $k \geqslant m^*$
    \[ KL(k) = D_{KL}(p_k \| p_{k+1}) = \int p_k(\bw) \log{\dfrac{p_k(\bw)}{p_{k+1}(\bw)}} d\bw \leqslant \varepsilon. \]
\end{definition}

Для пары нормальных распределений дивергенция Кульбака-Лейблера имеет достаточно простой вид. Предположим, что апостериорное распределение является нормальным, то есть $p_k(\bw) = \mathcal{N}\left( \bw | \bm_k, \bSigma_k \right)$. Руководствуясь эвристикой, что сходимость моментов такого распределения должна влечь за собой близость апостериорных распределений на схожих подвыборках, можно сформулировать следующее утверждение.

\begin{theorem}[Киселев, 2024]\label{theorem2}
    Пусть $\| \bm_{k+1} - \bm_k \|_2 \to 0$ и $\| \bSigma_{k+1} - \bSigma_k \|_{F} \to 0$ при $k \to \infty$. Тогда в модели с нормальным апостериорным распределением параметров определение KL-достаточного размера выборки является корректным. А именно, для любого $\varepsilon > 0$ найдется такой $m^*$, что для всех $k \geqslant m^*$ выполнено $KL(k) \leqslant \varepsilon$.
\end{theorem}

В настоящей работе предлагается в качестве меры сходства распределений использовать меру сходства s-score из [Адуенко]:
\[ \text{s-score}(p_1, p_2) = \dfrac{\int_{\bw} p_1(\bw) p_2(\bw) d\bw}{\max_{\mathbf{b}} \int_{\bw} p_1(\bw - \mathbf{b}) p_2(\bw) d\bw}. \]

\begin{definition}
    Зафиксируем некоторое положительное число $\varepsilon > 0$. Размер выборки $m^*$ называется \textbf{S-достаточным}, если для всех $k \geqslant m^*$
    \[ S(k) = \text{s-score}(p_k, p_{k+1}) \geqslant 1-\varepsilon. \]
\end{definition}

Как и в случае KL-достаточного размера выборки, в модели с нормальным апостериорным распределением есть возможность записать выражение для используемого критерия. Таким образом, можно сформулировать еще одно утверждение.

\begin{theorem}[Киселев, 2024]\label{theorem3}
    Пусть $\| \bm_{k+1} - \bm_k \|_2 \to 0$ при $k \to \infty$. Тогда в модели с нормальным апостериорным распределением параметров определение S-достаточного размера выборки является корректным. А именно, для любого $\varepsilon > 0$ найдется такой $m^*$, что для всех $k \geqslant m^*$ выполнено $S(k) \geqslant 1-\varepsilon$.
\end{theorem}

Пусть в модели линейной регрессии задано нормальное априорное распределение параметров. По свойству сопряженности априорного распределения и правдоподобия апостериорное распределение также является нормальным. Таким образом, мы приходим к одному из простейших примеров модели, для которой справедливы теоремы, представленные выше. На самом деле для линейной регрессии можно сформулировать более простые утверждения.

\begin{theorem}[Киселев, 2024]\label{theorem4}
    Пусть $\left\| \bX\T_{k+1} \bX_{k+1} - \bX\T_k \bX_k \right\|_2 = o(k^{-1/2})$ и $\lambda_{\min}\left( \bX\T_k \bX_k \right) = \Omega(1)$ при $k \to \infty$. Тогда в модели линейной регрессии с нормальным априорным распределением параметров $\| \bSigma_{k+1} - \bSigma_k \|_{F} \to 0$ при $k \to \infty$.
\end{theorem}

\begin{theorem}[Киселев, 2024]\label{theorem5}
    Пусть... Тогда в модели линейной регрессии с нормальным априорным распределением параметров $\| \bm_{k+1} - \bm_k \|_2 \to 0$ при $k \to \infty$.
\end{theorem}