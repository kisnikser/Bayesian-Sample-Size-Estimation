\section{Достаточный размер выборки не превосходит доступный}\label{sec2}

В этой главе будем считать, что достоверно $m^* \leqslant m$.

Рассмотрим выборку $\mathfrak{D}_k$ размера $k \leqslant m$. Оценим на ней параметры, используя метод максимума правдоподобия:
\[ \hat{\mathbf{w}}_{k} = \arg\max_{\mathbf{w}} L(\mathfrak{D}_k, \mathbf{w}). \]

Поскольку природа $\mathbf{w}$ нам неизвестна, для определения достаточности будем использовать функцию правдоподобия.

Когда в наличии имеется достаточно объектов, вполне естественно ожидать, что от одной реализации выборки к другой полученная оценка параметров не будет сильно меняться \cite{Joseph1997, Joseph1995}. То же можно сказать и про функцию правдоподобие. Таким образом, сформулируем, какой размер выборки можно считать достаточным.

\begin{definition}
    \label{sufficient-variance}
    Зафиксируем некоторое положительное число $\varepsilon > 0$. Размер выборки $m^*$ называется \textbf{D-достаточным}, если для любого $k \geqslant m^*$
    \[ D(k) = \mathbb{D}_{\mathfrak{D}_k} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \leqslant \varepsilon. \]
\end{definition}
\begin{note}
    В определении~\ref{sufficient-variance} вместо функции правдоподобия $L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$ можно рассматривать ее логарифм $l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$.
\end{note}

С другой стороны, когда в наличии имеется достаточно объектов, также вполне естественно, что при добавлении очередного объекта в рассмотрение полученная оценка параметров не будет сильно меняться. Сформулируем еще одно определение.

\begin{definition}
    \label{sufficient-difference}
    Зафиксируем некоторое положительное число $\varepsilon > 0$. Размер выборки $m^*$ называется \textbf{M-достаточным}, если для любого $k \geqslant m^*$ 
    \[ M(k) = \left| \mathbb{E}_{\mathfrak{D}_{k+1}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E}_{\mathfrak{D}_k} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \right| \leqslant \varepsilon. \]
\end{definition}
\begin{note}
    В определении~\ref{sufficient-difference} вместо функции правдоподобия $L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$ можно рассматривать ее логарифм $l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$.
\end{note}

\textcolor{red}{Как доказать корректность этих определений? А именно, почему такой размер выборки существует?}

Предположим, что $\mathbb{W} = \mathbb{R}^n$, т.е. параметры $\bw$ представляются в виде вектора. Напомним, что информацией Фишера называется матрица
\[ \left[\mathcal{I}(\bw)\right]_{ij} = - \mathbb{E}\left[ \dfrac{\partial^2 \log p(\by | \bX, \bw)}{\partial w_i \partial w_j} \right]. \]
Достаточно известным результатом является асимптотическая нормальность оценки максимума правдоподобия. 

\begin{proposition}
    Пусть $\hat{\bw}_k$~--- оценка максимума правдоподобия $\bw$. Тогда при определенных условиях регулярности (которые на практике чаще всего выполнены) имеет место следующая сходимость по распределению:
    \[ \sqrt{m}\left( \hat{\bw}_k - \bw \right) \xrightarrow{d} \mathcal{N}\left(0, \mathcal{I}^{-1}(\bw)\right), \]
    или, что равносильно,
    \[ \hat{\bw}_k \xrightarrow{d} \mathcal{N}\left(\bw, \left[m\mathcal{I}(\bw)\right]^{-1}\right). \]
\end{proposition}

Из сходимости по распределению в общем случае не следует сходимость моментов случайного вектора. Тем не менее, если предположить последнее, то в некоторых моделях можно доказать корректность предложенного нами определения M-достаточного размера выборки.

Для удобства обозначим параметры распределения $\hat{\bw}_k$ следующим образом: математическое ожидание $\mathbb{E}_{\mathfrak{D}_k}\hat{\bw}_k = \bm_k$ и матрица ковариации $\text{cov}(\hat{\bw}_k) = \bSigma_k$. Тогда имеет место следующая лемма.

\begin{lemma}\label{lemma1}
    Пусть $\| \bm_k - \bw \|_2 \to 0$ и $\| \bSigma_k - \left[m\mathcal{I}(\bw)\right]^{-1} \|_{F} \to 0$ при $k \to \infty$. Тогда в модели линейной регрессии определение M-достаточного размера выборки является корректным. А именно, найдется такой $m^*$, что для всех $k \geqslant m^*$ выполнено $M(k) \leqslant \varepsilon$.
\end{lemma}

По условию задана одна выборка. Поэтому в эксперименте нет возможности посчитать указанные в определениях математическое ожидание и дисперсию. Для их оценки воспользуемся техникой бутстрап. А именно, сгенерируем из заданной $\mathfrak{D}_m$ некоторое число $B$ подвыборок размера $k$ с возвращением. Для каждой из них получим оценку параметров $\hat{\mathbf{w}}_{k}$ и посчитаем значение $L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$. Для оценки будем использовать выборочное среднее и несмещенную выборочную дисперсию (по бутстрап-выборкам). \textcolor{red}{Как доказать <<хорошие>> свойства этих оценок?}